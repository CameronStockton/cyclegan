{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e7deee-2096-4a78-8249-81800aa0dbe2",
   "metadata": {},
   "source": [
    "analysis (model building and training), result, and discussion/conclusion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be917eb1-74ad-47a4-b100-a428d500af6b",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "A GAN consists of at least two neural networks: a generator model and a discriminator model. The generator is a neural network that creates the images. For our competition, you should generate images in the style of Monet. This generator is trained using a discriminator.\r\n",
    "\r\n",
    "The two models will work against each other, with the generator trying to trick the discriminator, and the discriminator trying to accurately classify the real vs. generated images.\r\n",
    "\r\n",
    "Your task is to build a GAN that generates 7,000 to 10,000 Monet-style images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0923289a-980e-4c50-9423-a3710dbe9770",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1424b6-b1c0-40da-ab1d-ab95cdada7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, losses, optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow_addons.layers import InstanceNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a66c6-85cf-45fc-9fa0-5e0821680485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image_paths, title):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        img = Image.open(img_path)\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "monet_images = random.sample(os.listdir('./monet_jpg'), 3)\n",
    "photo_images = random.sample(os.listdir('./photo_jpg'), 3)\n",
    "\n",
    "monet_image_paths = [os.path.join('./monet_jpg', img) for img in monet_images]\n",
    "photo_image_paths = [os.path.join('./photo_jpg', img) for img in photo_images]\n",
    "\n",
    "\n",
    "display_images(monet_image_paths, 'Monet Paintings')\n",
    "\n",
    "\n",
    "display_images(photo_image_paths, 'Photos')\n",
    "\n",
    "# These are for the generator\n",
    "def generate_images(generator, z_dim, num_images=10):\n",
    "    noise = tf.random.normal([num_images, z_dim])\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = (generated_images * 127.5) + 127.5\n",
    "    generated_images = generated_images.astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(num_images // 5 + 1, 5, i+1)\n",
    "        plt.imshow(generated_images[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_feature_maps(generator, z_dim, num_images=1):\n",
    "    noise = tf.random.normal([num_images, z_dim])\n",
    "    generated_images = generator(noise, training=False)\n",
    "\n",
    "    layer_outputs = [layer.output for layer in generator.layers if 'conv' in layer.name]\n",
    "    activation_model = tf.keras.models.Model(inputs=generator.input, outputs=layer_outputs)\n",
    "\n",
    "    feature_maps = activation_model.predict(noise)\n",
    "\n",
    "    for layer_name, feature_map in zip([layer.name for layer in generator.layers if 'conv' in layer.name], feature_maps):\n",
    "        size = feature_map.shape[1]\n",
    "        n_features = feature_map.shape[-1]\n",
    "        n_cols = n_features // 16\n",
    "        display_grid = np.zeros((size * n_cols, size * 16))\n",
    "\n",
    "        for col in range(n_cols):\n",
    "            for row in range(16):\n",
    "                channel_image = feature_map[0, :, :, col * 16 + row]\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                display_grid[col * size: (col + 1) * size, row * size: (row + 1) * size] = channel_image\n",
    "\n",
    "        scale = 20. / n_features\n",
    "        plt.figure(figsize=(scale * 16, scale * n_cols))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b78c5e-b58d-475e-a4ac-617e2d9e90bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     image_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m image_files]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_paths\n\u001b[1;32m----> 6\u001b[0m monet_image_paths \u001b[38;5;241m=\u001b[39m \u001b[43mget_all_image_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./monet_jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m photo_image_paths \u001b[38;5;241m=\u001b[39m get_all_image_paths(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./photo_jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m, in \u001b[0;36mget_all_image_paths\u001b[1;34m(image_dir)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_all_image_paths\u001b[39m(image_dir):\n\u001b[1;32m----> 2\u001b[0m     image_files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(image_dir)\n\u001b[0;32m      3\u001b[0m     image_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m image_files]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_paths\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def get_all_image_paths(image_dir):\n",
    "    image_files = os.listdir(image_dir)\n",
    "    image_paths = [os.path.join(image_dir, img) for img in image_files]\n",
    "    return image_paths\n",
    "\n",
    "monet_image_paths = get_all_image_paths('./monet_jpg')\n",
    "photo_image_paths = get_all_image_paths('./photo_jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5cd0b-0ea0-4a48-b0a2-c7eef7274dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path):\n",
    "    ''' Function that preprocesses the images.\n",
    "        Ensures that the image is RBG, and size (256, 256)\n",
    "        Normalizes the value\n",
    "    '''\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [256, 256])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "def augment_image(image):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.rot90(image, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    return image\n",
    "\n",
    "def prepare_and_augment(image_path):\n",
    "    image = preprocess_image(image_path)\n",
    "    return tf.data.Dataset.from_tensors(image).map(augment_image)\n",
    "\n",
    "\n",
    "monet_dataset = tf.data.Dataset.from_tensor_slices(monet_image_paths)\n",
    "monet_dataset = monet_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "photo_dataset = tf.data.Dataset.from_tensor_slices(photo_image_paths)\n",
    "photo_dataset = photo_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "for image in monet_dataset.take(1):\n",
    "    print(image.numpy().min(), image.numpy().max())\n",
    "\n",
    "for image in photo_dataset.take(1):\n",
    "    print(image.numpy().min(), image.numpy().max()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd429a-abca-494b-ac80-70c9c09ac459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16b5dc09-12d5-4c3b-8a8e-cd5fbf949266",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "Originally, i had built my own GAN that only upsamples and the discriminator downsamples. However, when i got into tuning, i discovered CycleGAN's method and so i created that architecture below. This is just for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622aecc4-0dbd-47fe-b5c8-54867c2cee9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_generator(z_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(z_dim,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((8, 8, 256)))\n",
    "\n",
    "    # Upsample to 16x16\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Upsample to 32x32\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Upsample to 64x64\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Upsample to 128x128\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Upsample to 256x256\n",
    "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator(image_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=image_shape))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 128x128\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 64x64\n",
    "    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 32x32\n",
    "    model.add(layers.Conv2D(512, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 16x16\n",
    "    model.add(layers.Conv2D(1024, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Flatten and output layer\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))  # No activation bc we'll use from_logits in loss function\n",
    "\n",
    "    return model\n",
    "\n",
    "generator_init = build_generator(z_dim)\n",
    "discriminator_init = build_discriminator((256, 256, 3))\n",
    "\n",
    "generator_init.summary()\n",
    "discriminator_init.summary()\n",
    "\n",
    "discriminator_init.compile(\n",
    "    optimizer=Adam(learning_rate=disc_learning_rate, beta_1=beta_1),\n",
    "    loss=BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e718eb-d23f-460f-9b55-c8496d11a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator, z_dim):\n",
    "    gan_input = layers.Input(shape=(z_dim,))\n",
    "    fake_image = generator(gan_input)\n",
    "    discriminator.trainable = False\n",
    "    gan_output = discriminator(fake_image)\n",
    "    \n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    \n",
    "    gan.compile(\n",
    "        optimizer=Adam(learning_rate=gen_learning_rate, beta_1=beta_1),\n",
    "        loss=BinaryCrossentropy(from_logits=True)\n",
    "    )\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc1997-abd7-410c-a74d-0ef3fbe9899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_init = build_gan(generator_init, discriminator_init, z_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd2ee3-634f-48ce-b2e4-a0a5214ab451",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60637e9-d6bd-4467-95e4-c6ed56544b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(gan, dataset, batch_size, z_dim, smooth_factor=0.1, epochs=1):\n",
    "    generator, discriminator = gan.layers[1:]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for real_images, _ in dataset:\n",
    "            batch_size = real_images.shape[0]\n",
    "\n",
    "            # Train real\n",
    "            real_labels = tf.ones((batch_size, 1)) * (1 - smooth_factor)  # Smooth labels for real images\n",
    "            d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
    "\n",
    "            # Train fake\n",
    "            noise = tf.random.normal(shape=(batch_size, z_dim))\n",
    "            fake_images = generator(noise)\n",
    "            fake_labels = tf.zeros((batch_size, 1)) + smooth_factor  # Smooth labels\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
    "\n",
    "            # Train the generator (thru gan)\n",
    "            noise = tf.random.normal(shape=(batch_size, z_dim))\n",
    "            gan_labels = tf.ones((batch_size, 1))\n",
    "            g_loss = gan.train_on_batch(noise, gan_labels)\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                generate_images(generator, z_dim, 3)\n",
    "\n",
    "            print(f\"D Loss Real: {d_loss_real}, D Loss Fake: {d_loss_fake}, G Loss: {g_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b7d42-27c2-4d72-8709-5303871872c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map the datasets to labels and combine\n",
    "monet_dataset = monet_dataset.map(lambda x: (x, tf.ones((1,))))\n",
    "monet_dataset_tfrec = monet_dataset_tfrec.map(lambda x: (x, tf.ones((1,))))\n",
    "\n",
    "photo_dataset = photo_dataset.map(lambda x: (x, tf.zeros((1,))))\n",
    "photo_dataset_tfrec = photo_dataset_tfrec.map(lambda x: (x, tf.zeros((1,))))\n",
    "\n",
    "# Combine\n",
    "combined_dataset = monet_dataset.concatenate(photo_dataset)\n",
    "combined_dataset_tfrec = monet_dataset_tfrec.concatenate(photo_dataset_tfrec)\n",
    "\n",
    "combined_dataset = combined_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "combined_dataset_tfrec = combined_dataset_tfrec.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e3d260-9444-4358-af08-abe34f325bd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_gan(gan_init, combined_dataset, batch_size, z_dim, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8bc8dc-a582-468f-82d1-56cb38ffdce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9b9b366-0010-42b8-8c93-5f6acfc8ea91",
   "metadata": {},
   "source": [
    "# Tuning\n",
    "\n",
    "It looks like the above architecture is a bit of overkill and severely overfits. We can tell this by the loss functions starting low and going even lower to essentially 0.\n",
    "\n",
    "We want the loss on D Real and D Fake to go down, meaning the discriminator is good at determining whether a photo is a Monet or not. Whereas we want our G loss to struggle at the beginning, going down meaning the discriminator can tell when a picture is fake. This will then cause the generator to create better pictures and fool the discriminator. Then the G loss should go up as the discriminator gets worse. And then there should ideally be a back and forth power struggle\n",
    "\n",
    "### Revision -> I was actually using a smaller subset of photos and thats why my model was overfitting. Now i am unsure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3741b-e177-4b70-9337-872c74176821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator_tuning(z_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(4 * 4 * 256, use_bias=False, input_shape=(z_dim, )))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((4, 4, 256)))\n",
    "\n",
    "    # Upsample to 8x8\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Upsample to 16x16\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Upsample to 32x32\n",
    "    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Upsample to 64x64\n",
    "    model.add(layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Upsample to 128x128\n",
    "    model.add(layers.Conv2DTranspose(8, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Upsample to 256x256\n",
    "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "def build_discriminator_tuning(image_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=image_shape))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 128x128\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 64x64\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 32x32\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 16x16\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Flatten\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))  # No activation because we'll use from_logits in the loss function\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b3f39-4db5-47de-bbe5-87a9b0791f37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator_tuning = build_generator_tuning(z_dim)\n",
    "discriminator_tuning = build_discriminator_tuning((256, 256, 3))\n",
    "\n",
    "generator_tuning.summary()\n",
    "discriminator_tuning.summary()\n",
    "\n",
    "discriminator_tuning.compile(\n",
    "    optimizer=Adam(learning_rate=disc_learning_rate, beta_1=beta_1),\n",
    "    loss=BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def24504-072a-49f2-86a2-3f836a43315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_tuning = build_gan(generator_tuning, discriminator_tuning, z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780622b5-87c8-46f3-805c-8e7557d32617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_gan(gan_tuning, combined_dataset, batch_size, z_dim, smooth_factor, epochs=4000)\n",
    "generate_images(generator_tuning, z_dim, num_images=10)\n",
    "visualize_feature_maps(generator_tuning, z_dim, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df96189-9c8d-474a-8130-66f9a9d170c0",
   "metadata": {},
   "source": [
    "# CycleGAN Architecture\n",
    "\n",
    "Please note the hyperparameters were moved below here. They could be referenced above and thats why the code isnt running. This is the important code. I also did not run a GridSearch as it is pretty computationally expensive. I spent a few days tuning this by trial and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51a1dd-1a71-4779-ad0e-8e460999a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "z_dim = 600\n",
    "disc_learning_rate = 0.002\n",
    "gen_learning_rate = 0.002\n",
    "beta_1 = 0.5 # change this to 0.9 -> slower but more controlled convergence ideally maybe 0.8\n",
    "batch_size = 32\n",
    "smooth_factor = 0.03\n",
    "lambda_cycle = 10 # Weight for cycle consistency loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2465934-c59a-487d-9556-fd2e1ba60070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator_cyclegan():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Initial convolution layer\n",
    "    model.add(layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same', input_shape=(256, 256, 3)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "\n",
    "    # Downsampling\n",
    "    model.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "\n",
    "    model.add(layers.Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "\n",
    "    model.add(layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same',kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "\n",
    "    model.add(layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same',kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "\n",
    "    model.add(layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same',kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "\n",
    "    model.add(layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "\n",
    "    model.add(layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "\n",
    "    # Upsampling\n",
    "    model.add(layers.Conv2DTranspose(512, (4, 4), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(512, (4, 4), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(512, (4, 4), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(512, (4, 4), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(InstanceNormalization())\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same', activation='tanh'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_discriminator_cyclegan(image_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=image_shape))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 128x128\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 64x64\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 32x32\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 16x16\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Flatten\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))  # No activation because we'll use from_logits in the loss function\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_images_cyclegan(generator, test_input):\n",
    "    generated_images = generator(test_input, training=False)\n",
    "    generated_images = (generated_images * 127.5) + 127.5\n",
    "    generated_images = generated_images.numpy().astype(np.uint8)\n",
    "\n",
    "    test_input = (test_input * 127.5) + 127.5\n",
    "    test_input = test_input.numpy().astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(2, generated_images.shape[0], i+1)\n",
    "        plt.imshow(test_input[i])\n",
    "        plt.title(\"Original\" if i == 0 else \"\")  # Only add the title to the first image for clarity\n",
    "        plt.axis('off')\n",
    "\n",
    "        # AI Generated images\n",
    "        plt.subplot(2, generated_images.shape[0], i + 1 + generated_images.shape[0])\n",
    "        plt.imshow(generated_images[i])\n",
    "        plt.title(\"AI Generated\" if i == 0 else \"\")  # Only add the title to the first image for clarity\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def visualize_feature_maps_cyclegan(generator, test_input):\n",
    "    generated_images = generator(test_input, training=False)\n",
    "\n",
    "    layer_outputs = [layer.output for layer in generator.layers if 'conv' in layer.name]\n",
    "    activation_model = tf.keras.models.Model(inputs=generator.input, outputs=layer_outputs)\n",
    "\n",
    "    feature_maps = activation_model.predict(test_input)\n",
    "\n",
    "    for layer_name, feature_map in zip([layer.name for layer in generator.layers if 'conv' in layer.name], feature_maps):\n",
    "        size = feature_map.shape[1]\n",
    "        n_features = feature_map.shape[-1]\n",
    "        n_cols = n_features // 16\n",
    "        display_grid = np.zeros((size * n_cols, size * 16))\n",
    "\n",
    "        for col in range(n_cols):\n",
    "            for row in range(16):\n",
    "                channel_image = feature_map[0, :, :, col * 16 + row]\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                display_grid[col * size: (col + 1) * size, row * size: (row + 1) * size] = channel_image\n",
    "\n",
    "        scale = 20. / n_features\n",
    "        plt.figure(figsize=(scale * 16, scale * n_cols))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a771086-8010-4c83-b6b8-b32a06d70095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing research into CycleGAN and want to recreate that architecture\n",
    "photo_to_monet_generator = build_generator_cyclegan()\n",
    "monet_to_photo_generator = build_generator_cyclegan()\n",
    "photo_discriminator = build_discriminator_cyclegan((256, 256, 3))\n",
    "monet_discriminator = build_discriminator_cyclegan((256, 256, 3))\n",
    "\n",
    "gen_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    gen_learning_rate,\n",
    "    decay_steps=5000,\n",
    "    decay_rate=0.94,\n",
    "    staircase=True)\n",
    "disc_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    disc_learning_rate,\n",
    "    decay_steps=4000,\n",
    "    decay_rate=0.85,\n",
    "    staircase=True)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=gen_learning_rate, beta_1=beta_1)\n",
    "discriminator_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=disc_lr_schedule, beta_1=beta_1)\n",
    "\n",
    "photo_discriminator.compile(optimizer=discriminator_optimizer, loss=losses.BinaryCrossentropy(from_logits=True))\n",
    "monet_discriminator.compile(optimizer=discriminator_optimizer, loss=losses.BinaryCrossentropy(from_logits=True))\n",
    "\n",
    "#generator_optimizer.build(photo_to_monet_generator.trainable_variables)\n",
    "#generator_optimizer.build(monet_to_photo_generator.trainable_variables)\n",
    "#discriminator_optimizer.build(photo_discriminator.trainable_variables)\n",
    "#discriminator_optimizer.build(monet_discriminator.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d5dcdd-92e4-49c3-8e1b-02751d9a8526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_consistency_loss(real_images, generated_images, lambda_cycle=lambda_cycle):\n",
    "    return lambda_cycle * tf.reduce_mean(tf.abs(real_images - generated_images))\n",
    "\n",
    "def generator_loss(disc_generated_output, lambda_cycle=lambda_cycle/2):\n",
    "    return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    total_disc_loss = (real_loss + generated_loss) * 0.5\n",
    "    return tf.reduce_mean(total_disc_loss)\n",
    "\n",
    "def identity_loss(real_image, same_image, lambda_identity=lambda_cycle/2):\n",
    "    return lambda_identity * tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "\n",
    "def train_cyclegan(photo_dataset, monet_dataset, epochs=1):\n",
    "    sample_photos = next(iter(photo_dataset.take(5)))\n",
    "    sample_monets = next(iter(monet_dataset.take(5)))\n",
    "    for epoch in range(epochs):\n",
    "        photo_gen_loss_sum = monet_gen_loss_sum = photo_disc_loss_sum = monet_disc_loss_sum = cycle_loss_sum = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for photo_images, monet_images in zip(photo_dataset, monet_dataset):\n",
    "            num_batches += 1\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # Generate images\n",
    "                fake_monet = photo_to_monet_generator(photo_images, training=True)\n",
    "                cycled_photo = monet_to_photo_generator(fake_monet, training=True)\n",
    "\n",
    "                fake_photo = monet_to_photo_generator(monet_images, training=True)\n",
    "                cycled_monet = photo_to_monet_generator(fake_photo, training=True)\n",
    "\n",
    "                # Generate Real Image\n",
    "                same_monet = photo_to_monet_generator(monet_images, training=True)\n",
    "                same_photo = monet_to_photo_generator(photo_images, training=True)\n",
    "\n",
    "                # Discriminator output\n",
    "                disc_real_photo = photo_discriminator(photo_images, training=True)\n",
    "                disc_fake_photo = photo_discriminator(fake_photo, training=True)\n",
    "\n",
    "                disc_real_monet = monet_discriminator(monet_images, training=True)\n",
    "                disc_fake_monet = monet_discriminator(fake_monet, training=True)\n",
    "\n",
    "                # Calculate losses\n",
    "                photo_gen_loss = generator_loss(disc_fake_monet)\n",
    "                monet_gen_loss = generator_loss(disc_fake_photo)\n",
    "                photo_disc_loss = discriminator_loss(disc_real_photo, disc_fake_photo)\n",
    "                monet_disc_loss = discriminator_loss(disc_real_monet, disc_fake_monet)\n",
    "                total_cycle_loss = cycle_consistency_loss(photo_images, cycled_photo) + cycle_consistency_loss(monet_images, cycled_monet)\n",
    "\n",
    "                # Calculate identity loss\n",
    "                photo_identity_loss = identity_loss(photo_images, same_photo, lambda_identity=lambda_cycle/2) # lamdba_identity makes sure the image looks like itself\n",
    "                monet_identity_loss = identity_loss(monet_images, same_monet, lambda_identity=lambda_cycle/2)\n",
    "                \n",
    "                # Total generator loss with identity loss\n",
    "                total_photo_gen_loss = photo_gen_loss + total_cycle_loss + photo_identity_loss\n",
    "                total_monet_gen_loss = monet_gen_loss + total_cycle_loss + monet_identity_loss\n",
    "\n",
    "                # Accumulate losses for logging\n",
    "                photo_gen_loss_sum += photo_gen_loss.numpy()\n",
    "                monet_gen_loss_sum += monet_gen_loss.numpy()\n",
    "                photo_disc_loss_sum += photo_disc_loss.numpy()\n",
    "                monet_disc_loss_sum += monet_disc_loss.numpy()\n",
    "                cycle_loss_sum += total_cycle_loss.numpy()\n",
    "\n",
    "            # Calculate the gradients and apply them\n",
    "            photo_generator_gradients = tape.gradient(total_photo_gen_loss, photo_to_monet_generator.trainable_variables)\n",
    "            monet_generator_gradients = tape.gradient(total_monet_gen_loss, monet_to_photo_generator.trainable_variables)\n",
    "            photo_discriminator_gradients = tape.gradient(photo_disc_loss, photo_discriminator.trainable_variables)\n",
    "            monet_discriminator_gradients = tape.gradient(monet_disc_loss, monet_discriminator.trainable_variables)\n",
    "\n",
    "            generator_optimizer.apply_gradients(zip(photo_generator_gradients, photo_to_monet_generator.trainable_variables))\n",
    "            generator_optimizer.apply_gradients(zip(monet_generator_gradients, monet_to_photo_generator.trainable_variables))\n",
    "            discriminator_optimizer.apply_gradients(zip(photo_discriminator_gradients, photo_discriminator.trainable_variables))\n",
    "            discriminator_optimizer.apply_gradients(zip(monet_discriminator_gradients, monet_discriminator.trainable_variables))\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\"Generated images at epoch\", epoch + 1)\n",
    "            generate_images_cyclegan(photo_to_monet_generator, sample_photos)\n",
    "            generate_images_cyclegan(monet_to_photo_generator, sample_monets)\n",
    "\n",
    "            \n",
    "        # Print the average losses\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        print(f'    Photo Generator Loss: {photo_gen_loss_sum / num_batches}')\n",
    "        print(f'    Monet Generator Loss: {monet_gen_loss_sum / num_batches}')\n",
    "        print(f'    Photo Discriminator Loss: {photo_disc_loss_sum / num_batches}')\n",
    "        print(f'    Monet Discriminator Loss: {monet_disc_loss_sum / num_batches}')\n",
    "        print(f'    Photo Identity Loss: {photo_identity_loss / num_batches}')\n",
    "        print(f'    Monet Identity Loss: {monet_identity_loss / num_batches}')\n",
    "        print(f'    Cycle Consistency Loss: {cycle_loss_sum / num_batches}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e303a192-62bc-42f8-93d7-3a96a93f4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "monet_dataset_cyclegan = tf.data.Dataset.from_tensor_slices(monet_image_paths)\n",
    "monet_dataset_cyclegan_augmented = monet_dataset_cyclegan.flat_map(prepare_and_augment)\n",
    "monet_dataset_cyclegan_augmented = monet_dataset_cyclegan_augmented.shuffle(buffer_size=1024).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "photo_dataset_cyclegan = tf.data.Dataset.from_tensor_slices(photo_image_paths)\n",
    "photo_dataset_cyclegan_augmented = photo_dataset_cyclegan.flat_map(prepare_and_augment)\n",
    "photo_dataset_cyclegan_augmented = photo_dataset_cyclegan_augmented.shuffle(buffer_size=1024).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ceabf6-971b-42fa-b574-f5ed127c4583",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_cyclegan(photo_dataset_cyclegan_augmented, monet_dataset_cyclegan_augmented, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664d5026-82eb-4b59-b68f-8f7f543df66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "i = 1\n",
    "for img in photo_dataset_cyclegan_augmented:\n",
    "    prediction = photo_to_monet_generator(img, training=False)[0].numpy()\n",
    "    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
    "    if i <= 4:  # Display and save the first four images\n",
    "        plt.subplot(1, 4, i)\n",
    "        plt.imshow(prediction)\n",
    "        plt.axis('off')\n",
    "\n",
    "        im = Image.fromarray(prediction)\n",
    "        im.save(\"./final_images/\" + str(i) + \".jpg\")\n",
    "\n",
    "    i += 1\n",
    "    if i > 5:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c542e-cb54-4176-823a-5d0eb86dc3cf",
   "metadata": {},
   "source": [
    "# Result\n",
    "\n",
    "Unfortunately, my GAN did not get to the point of producing good images. The Cycle Consistency Loss is always over 10 (which is weighted by 10 so the loss is rarely less than 1). It's like the Losses arent applied to the gradients properly, but i've double checked the training loop multiple times. I use Keras's Example CycleGAN to help tune and debug the model, but yet my model doesnt perform nearly as close. They can train for 5 epochs and it produces a proper picture, i train for 5000 (10 hours overnight) and i get static."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd313a-24af-41ce-bb06-aa0628dc7a80",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion\n",
    "\n",
    "I tried dozen of hyperparameter permutations. I tuned for 2 days. At one point, i was getting the generator to produce images but it still suffered from mode collapse, where the generator had learned to draw 1 specific picture. Then in an effort to fix that, I converted to trying to create a CycleGAN copy where i found their architecture in literature. I do a few things differently that may effect the model like not optimally initializing my neural network layers and my instance normalizations. I also had a much smaller dataset, only 300 Monet's. I think that could've been a big factor, so i had implemented augmentation to increase the data diversity. Overall, this was very frustrating due to constant mode collapse but i learned a lot!\n",
    "\n",
    "### EDIT: I'm dumb! In my data loading i was only using 3 samples!!! CRAZY! I retrained on 15 epochs and it took so much longer per epoch so that makes sense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34cd636-5a84-4c8b-a980-fdb0c60b38d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c0af6-6446-416b-9df6-16c5860f2112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17635ee6-daab-403f-ad77-472e9679e097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7d6e0-aa2c-4ec8-b046-121b8a0de48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d96718b2-e017-4067-93f1-e2374a9fe5c6",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- https://developers.google.com/machine-learning/gan/applications\n",
    "- https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf\n",
    "- https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial#Visualize-our-Monet-esque-photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7c6a9-7fb6-469e-be9b-2637aa26d387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
